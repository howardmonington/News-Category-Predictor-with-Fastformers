{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88210999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow.keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a9fa9",
   "metadata": {},
   "source": [
    "# Implementing Fastformers\n",
    "In this project, I'm going to be implementing Fastformers, as introduced in a research paper titled <b> Fastformer: Additive Attention Can Be All You Need </b> which was published on September 5th, 2021. The full paper can be found [here](https://arxiv.org/pdf/2108.09084v6.pdf).\n",
    "\n",
    "Transformers have proven to be very successful in many state-of-the-art pre-trained language models in NLP and in vision-related tasks. Transformers implement a concept called self-attention, which allows them to model the contexts within an input sequence. One limitation of today's transformers is that, because they compute the dot-product between the input representations at each pair of positions, their complexity is quadratic to the input sequence length. This makes it difficult for standard Transformer models to efficiently handle long input sequences. The concept proposed in this research paper to solve the quadratic complexity problem is <b> additive attention</b>, which can achieve effective context modeling in linear complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d9dfb",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "I'm going to be using a News Category Dataset which can be found [here](https://www.kaggle.com/rmisra/news-category-dataset). This dataset contains around 200k news headlines and short descriptions from the year 2012 to 2018 obtained from HuffPost. It contains 41 separate categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f994d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('archive/News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afe314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44624e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total categories: 41\n",
      "category\n",
      "ARTS               1509\n",
      "ARTS & CULTURE     1339\n",
      "BLACK VOICES       4528\n",
      "BUSINESS           5937\n",
      "COLLEGE            1144\n",
      "COMEDY             5175\n",
      "CRIME              3405\n",
      "CULTURE & ARTS     1030\n",
      "DIVORCE            3426\n",
      "EDUCATION          1004\n",
      "ENTERTAINMENT     16058\n",
      "ENVIRONMENT        1323\n",
      "FIFTY              1401\n",
      "FOOD & DRINK       6226\n",
      "GOOD NEWS          1398\n",
      "GREEN              2622\n",
      "HEALTHY LIVING     6694\n",
      "HOME & LIVING      4195\n",
      "IMPACT             3459\n",
      "LATINO VOICES      1129\n",
      "MEDIA              2815\n",
      "MONEY              1707\n",
      "PARENTING          8677\n",
      "PARENTS            3955\n",
      "POLITICS          32739\n",
      "QUEER VOICES       6314\n",
      "RELIGION           2556\n",
      "SCIENCE            2178\n",
      "SPORTS             4884\n",
      "STYLE              2254\n",
      "STYLE & BEAUTY     9649\n",
      "TASTE              2096\n",
      "TECH               2082\n",
      "THE WORLDPOST      3664\n",
      "TRAVEL             9887\n",
      "WEDDINGS           3651\n",
      "WEIRD NEWS         2670\n",
      "WELLNESS          17827\n",
      "WOMEN              3490\n",
      "WORLD NEWS         2177\n",
      "WORLDPOST          2579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cates = df.groupby('category')\n",
    "print(\"total categories:\", cates.ngroups)\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb47c9a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as shown above, THE WORLDPOST and WORLDPOST should be the same category, so merge them\n",
    "df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb789dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using headlines and short_description as input X\n",
    "df['text'] = df.headline + \" \" + df.short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d132129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV She left her husband. He killed their children. Just another day in America.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc43bf",
   "metadata": {},
   "source": [
    "## Label Encoding and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877e5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary to map the category types to numbers and the numbers to category types.\n",
    "# category to id\n",
    "categories = df.groupby('category').size().index.tolist()\n",
    "\n",
    "category_int = {}\n",
    "int_category = {}\n",
    "for i, k in enumerate(categories):\n",
    "    category_int.update({k:i})\n",
    "    int_category.update({i:k})\n",
    "    \n",
    "df['label'] = df['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92935675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921159bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of characters is: 173.25\n",
      "The max number of characters is: 1487\n"
     ]
    }
   ],
   "source": [
    "average_length = df['text'].apply(len).mean()\n",
    "max_length = df['text'].apply(len).max()\n",
    "print(f'The average number of characters is: {round(average_length,2)}\\nThe max number of characters is: {round(max_length,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89d746",
   "metadata": {},
   "source": [
    "This takes every string in the 'text' column and converts it to tokens. The tokenizer `wordpunct_tokenize` is based on a simple regexp tokenization. It will split pretty much all special symbols and treat them as separate units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd4d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(df['label'])\n",
    "\n",
    "text = []\n",
    "for row in df['text']:\n",
    "    text.append(wordpunct_tokenize(row.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68bb6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving all unique tokens a number in a word_dict dictionary\n",
    "word_dict={'PADDING':0}\n",
    "for sent in text:\n",
    "    for token in sent:\n",
    "        if token not in word_dict:\n",
    "            word_dict[token] = len(word_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27aa6fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the average number of tokens in each sentence\n",
    "tokens_per_sentence = []\n",
    "for sent in text:\n",
    "    tokens_per_sentence.append(len(sent))\n",
    "\n",
    "average_length = round(sum(tokens_per_sentence)/len(tokens_per_sentence),2)\n",
    "average_length   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ed4a8",
   "metadata": {},
   "source": [
    "## Making all Sequences into Uniform Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299ac49",
   "metadata": {},
   "source": [
    "Here I am converting all of the tokenized sentences to their numerical representations. Then I am trimming / padding each sentence to be of the same uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de606715",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 32\n",
    "news_words = []\n",
    "for sent in text:\n",
    "    sample = []\n",
    "    for token in sent:\n",
    "        sample.append(word_dict[token])\n",
    "    sample = sample[:MAX_SENT_LENGTH]\n",
    "    news_words.append(sample+[0]*(MAX_SENT_LENGTH-len(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad533a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the numerical tokens and labels to dtype int32\n",
    "news_words = np.array(news_words,dtype='int32')\n",
    "label = np.array(label,dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72df749",
   "metadata": {},
   "source": [
    "## Splitting into a Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77eabea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180767"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(label) * 0.9)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9580faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting an array with numbers for the train and test index values\n",
    "index = np.arange(len(label))\n",
    "train_size = int(len(label) * 0.9)\n",
    "train_index = index[:train_size]\n",
    "np.random.shuffle(train_index)\n",
    "test_index=index[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f492490",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Creating the Fastformer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac4c48",
   "metadata": {},
   "source": [
    "Here is the structure of a Fastformer as shown in the research paper: <img src = \"Images/Fastformer_structure.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec7921",
   "metadata": {},
   "source": [
    "Here is a breakdown of the image:\n",
    "<ol>\n",
    "    <li> The input embedding matrix is transformed into the query, key, and value sequences using three independent linear transformation layers</li>\n",
    "    <li> The additive attention mechanism is used to summarize the input attention query matrix into a global query vector </li>\n",
    "    <li> The interaction between attention key and the global query vector is modeled via element-wise product to learn a global context-aware key matrix. </li>\n",
    "        <i> Element-wise product is used instead of addition in order to model the non-linear relations between the two vectors because addition cannot differ the influence of the global query on different keys (which is not beneficial for context understanding) </i>\n",
    "    <li> The global context-aware key matrix is summarized into a global key vector via additive attention</li>\n",
    "    <li> The element-wise product is used to aggregate the global key and attention value</li>\n",
    "    <li> The global key and attention value are further processed by a linear transformation to compute the global context-aware attention value</li>\n",
    "    <li> The original attention query and the global context-aware attention value are added to form the final output</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1468e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fastformer(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.now_input_shape=None\n",
    "        super(Fastformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.now_input_shape=input_shape\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True) \n",
    "        self.Wq = self.add_weight(name='Wq', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.Wk = self.add_weight(name='Wk', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        self.WP = self.add_weight(name='WP', \n",
    "                                  shape=(self.output_dim,self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        \n",
    "        super(Fastformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if len(x) == 2:\n",
    "            Q_seq,K_seq = x\n",
    "        elif len(x) == 4:\n",
    "            Q_seq,K_seq,Q_mask,K_mask = x #different mask lengths, reserved for cross attention\n",
    "\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)        \n",
    "        Q_seq_reshape = K.reshape(Q_seq, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        Q_att=  K.permute_dimensions(K.dot(Q_seq_reshape, self.Wq),(0,2,1))/ self.size_per_head**0.5\n",
    "\n",
    "        if len(x)  == 4:\n",
    "            Q_att = Q_att-(1-K.expand_dims(Q_mask,axis=1))*1e8\n",
    "\n",
    "        Q_att = K.softmax(Q_att)\n",
    "        Q_seq = K.reshape(Q_seq, (-1,self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        \n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1,self.now_input_shape[1][1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "\n",
    "        Q_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(Q_att)\n",
    "        global_q = K.sum(multiply([Q_att, Q_seq]),axis=2)\n",
    "        \n",
    "        global_q_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[1][1],axis=2))(global_q)\n",
    "\n",
    "        QK_interaction = multiply([K_seq, global_q_repeat])\n",
    "        QK_interaction_reshape = K.reshape(QK_interaction, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        K_att = K.permute_dimensions(K.dot(QK_interaction_reshape, self.Wk),(0,2,1))/ self.size_per_head**0.5\n",
    "        \n",
    "        if len(x)  == 4:\n",
    "            K_att = K_att-(1-K.expand_dims(K_mask,axis=1))*1e8\n",
    "            \n",
    "        K_att = K.softmax(K_att)\n",
    "\n",
    "        K_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(K_att)\n",
    "\n",
    "        global_k = K.sum(multiply([K_att, QK_interaction]),axis=2)\n",
    "     \n",
    "        global_k_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[0][1],axis=2))(global_k)\n",
    "        #Q=V\n",
    "        QKQ_interaction = multiply([global_k_repeat, Q_seq])\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        QKQ_interaction = K.dot(QKQ_interaction, self.WP)\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head,self.size_per_head))\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = QKQ_interaction+Q_seq\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        #many operations can be optimized if higher versions are used. \n",
    "        \n",
    "        return QKQ_interaction\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d0a7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180767 samples\n",
      "Epoch 1/2\n",
      "180767/180767 [==============================] - 385s 2ms/sample - loss: 1.5930 - acc: 0.5670\n",
      "Epoch 2/2\n",
      "180767/180767 [==============================] - 382s 2ms/sample - loss: 0.9652 - acc: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 22:35:53.515126: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2021-09-14 22:35:53.515687: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2021-09-14 22:35:53.516842: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] model_pruner failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'fastformer_1/lambda_5/concat' has self cycle fanin 'fastformer_1/lambda_5/concat'.\n",
      "2021-09-14 22:35:53.519253: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'fastformer_1/lambda_5/concat' has self cycle fanin 'fastformer_1/lambda_5/concat'.\n",
      "2021-09-14 22:35:53.519638: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:533] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.\n",
      "2021-09-14 22:35:53.519998: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\n",
      "2021-09-14 22:35:53.520333: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20086/20086 [==============================] - 4s 198us/sample\n",
      "0.7085544144846351\n",
      "Train on 180767 samples\n",
      "Epoch 1/2\n",
      "180767/180767 [==============================] - 380s 2ms/sample - loss: 0.6366 - acc: 0.8062\n",
      "Epoch 2/2\n",
      "180767/180767 [==============================] - 380s 2ms/sample - loss: 0.4455 - acc: 0.8608\n",
      "20086/20086 [==============================] - 4s 177us/sample\n",
      "0.6706256158320152\n"
     ]
    }
   ],
   "source": [
    "tensorflow.keras.backend.clear_session() \n",
    "\n",
    "text_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "qmask=Lambda(lambda x:  K.cast(K.cast(x,'bool'),'float32'))(text_input)\n",
    "\n",
    "word_emb = Embedding(len(word_dict),256, trainable=True)(text_input)\n",
    "word_emb = Dropout(0.2)(word_emb)\n",
    "\n",
    "hidden_word_emb = Fastformer(16,16)([word_emb,word_emb,qmask,qmask])\n",
    "hidden_word_emb = Dropout(0.2)(hidden_word_emb)\n",
    "hidden_word_emb = LayerNormalization()(add([word_emb,hidden_word_emb])) \n",
    "\n",
    "hidden_word_emb_layer2 = Fastformer(16,16)([hidden_word_emb,hidden_word_emb,qmask,qmask])\n",
    "hidden_word_emb_layer2 = Dropout(0.2)(hidden_word_emb_layer2)\n",
    "hidden_word_emb_layer2 = LayerNormalization()(add([hidden_word_emb,hidden_word_emb_layer2]))\n",
    "\n",
    "word_att = Flatten()(Dense(1)(hidden_word_emb_layer2))\n",
    "word_att = Activation('softmax')(word_att)\n",
    "text_emb = Dot((1, 1))([hidden_word_emb_layer2 , word_att])\n",
    "classifier = Dense(40, activation='softmax')(text_emb)\n",
    "                                      \n",
    "model = Model([text_input], [classifier])\n",
    "model.compile(loss=['categorical_crossentropy'],optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "for i in range(2):\n",
    "    model.fit(news_words[train_index],to_categorical(label)[train_index], shuffle=True, batch_size=64, epochs=2, verbose=1)\n",
    "    y_pred = model.predict([news_words[test_index] ], batch_size=128, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = label[test_index]\n",
    "    # 'weighted' means that it calculates metrics for each label and finds their average weighted by the\n",
    "    # number of true instances for each label\n",
    "    report = f1_score(y_true, y_pred, average='weighted')  \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81c54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
