# News Category Predictor with Fastformers

#### -- Project Status: [Completed]

## Project Intro/Objective
In this project, I'm going to be implementing Fastformers, as introduced in a research paper titled Fastformer: Additive Attention Can Be All You Need which was published on September 5th, 2021. The full paper can be found [here](https://arxiv.org/pdf/2108.09084v6.pdf). Using the Fastformers architecture, I was able to train a model to predict News Categories with an accuracy of 86%. 


### Methods Used
* Fastformers
* Transformers
* Deep Learning

### Technologies
* Python
* Jupyter
* Numpy
* Pandas
* Keras
* Sci-Kit Learn


## Project Description
Transformers have proven to be very successful in many state-of-the-art pre-trained language models in NLP and in vision-related tasks. Transformers implement a concept called self-attention, which allows them to model the contexts within an input sequence. One limitation of today's transformers is that, because they compute the dot-product between the input representations at each pair of positions, their complexity is quadratic to the input sequence length. This makes it difficult for standard Transformer models to efficiently handle long input sequences. The concept proposed in this research paper to solve the quadratic complexity problem is additive attention, which can achieve effective context modeling in linear complexity.


## Featured Notebooks/Analysis/Deliverables
* [Notebook](https://github.com/lukemonington/fastformers_for_nlp/blob/main/News%20Category%20Predictor%20with%20Fastformers.ipynb)


## Contributing Members

**[Luke Monington](https://github.com/lukemonington)**

## Contact
* I can be reached at lukemonington3@gmail.com.
